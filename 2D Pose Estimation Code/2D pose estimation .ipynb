{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a53c8f-227c-407b-84cb-32bdbd99d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from math import atan2, cos, sin, sqrt, pi\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4567975-6296-477b-ba39-aade94c041b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawAxis(img, p_, q_, color, scale):\n",
    "  p = list(p_)\n",
    "  q = list(q_)\n",
    "  ## [visualization1]\n",
    "  angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians\n",
    "  hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))\n",
    "  # Here we lengthen the arrow by a factor of scale\n",
    "  q[0] = p[0] - scale * hypotenuse * cos(angle)\n",
    "  q[1] = p[1] - scale * hypotenuse * sin(angle)\n",
    "  cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 3, cv2.LINE_AA)\n",
    "  # create the arrow hooks\n",
    "  p[0] = q[0] + 9 * cos(angle + pi / 4)\n",
    "  p[1] = q[1] + 9 * sin(angle + pi / 4)\n",
    "  cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 3, cv2.LINE_AA)\n",
    "  p[0] = q[0] + 9 * cos(angle - pi / 4)\n",
    "  p[1] = q[1] + 9 * sin(angle - pi / 4)\n",
    "  cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 3, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f10772-5d3b-44d4-b553-4b5cda6c21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrientation(pts, img):\n",
    "  ## [pca]\n",
    "  # Construct a buffer used by the pca analysis\n",
    "  sz = len(pts)\n",
    "  data_pts = np.empty((sz, 2), dtype=np.float64)\n",
    "  for i in range(data_pts.shape[0]):\n",
    "    data_pts[i,0] = pts[i,0,0]\n",
    "    data_pts[i,1] = pts[i,0,1]\n",
    " \n",
    "  # Perform PCA analysis\n",
    "  mean = np.empty((0))\n",
    "  mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)\n",
    "  # Store the center of the object\n",
    "  cntr = (int(mean[0,0]), int(mean[0,1]))\n",
    "  ## [pca]\n",
    " \n",
    "  ## [visualization]\n",
    "  # Draw the principal components\n",
    "  cv2.circle(img, cntr, 3, (255, 0, 255), 2)\n",
    "  p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])\n",
    "  p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])\n",
    "  # drawAxis(img, cntr, p1, (255, 255, 0), 1)\n",
    "  # drawAxis(img, cntr, p2, (0, 0, 255), 5)\n",
    "  angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians\n",
    "  ## [visualization]\n",
    "   # Label with the rotation angle\n",
    "  label = \"  Rotation Angle: \" + str(int(np.rad2deg(angle)) ) + \" degrees\"\n",
    "  # textbox = cv2.rectangle(img, (cntr[0], cntr[1]-25), (cntr[0] + 250, cntr[1] + 10), (255,255,255), -1)\n",
    "  cv2.putText(img, label, (cntr[0], cntr[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "  return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ce7786-8666-4f64-ac03-802bd90b3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_angle(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, bw = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    for i, c in enumerate(contours):\n",
    "        # Calculate the area of each contour\n",
    "        area = cv2.contourArea(c)\n",
    "        # Ignore contours that are too small or too large\n",
    "        if area < 3700 or 100000 < area:\n",
    "             continue\n",
    "        # Draw each contour only for visualisation purposes\n",
    "        cv2.drawContours(img, contours, i, (0, 0, 255), 2)\n",
    "        # Find the orientation of each shape\n",
    "        getOrientation(c, img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171e36b0-a1df-4fea-9dd2-1d985cd3e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The demo requires Depth camera with Color sensor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiwar\\Desktop\\Personal\\Everything related to project\\Pose Estimation\\2D pose estimation .ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Show images\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m cv2\u001b[39m.\u001b[39mnamedWindow(\u001b[39m'\u001b[39m\u001b[39mRealSense\u001b[39m\u001b[39m'\u001b[39m, cv2\u001b[39m.\u001b[39mWINDOW_AUTOSIZE)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m color_image\u001b[39m=\u001b[39mtell_angle(color_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mRealSense\u001b[39m\u001b[39m'\u001b[39m, color_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\tiwar\\Desktop\\Personal\\Everything related to project\\Pose Estimation\\2D pose estimation .ipynb Cell 5\u001b[0m in \u001b[0;36mtell_angle\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m contours, _ \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mfindContours(bw, cv2\u001b[39m.\u001b[39mRETR_LIST, cv2\u001b[39m.\u001b[39mCHAIN_APPROX_NONE)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(contours):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Calculate the area of each contour\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     area \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcontourArea(c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Ignore contours that are too small or too large\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiwar/Desktop/Personal/Everything%20related%20to%20project/Pose%20Estimation/2D%20pose%20estimation%20.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m area \u001b[39m<\u001b[39m \u001b[39m3700\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39m100000\u001b[39m \u001b[39m<\u001b[39m area:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        # depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        \n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        \n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        color_image=tell_angle(color_image)\n",
    "        cv2.imshow('RealSense', color_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1cd8b-8203-4daa-8c60-0b0780024d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
